# Step-by-Step Guide: Writing Tests for Sub-Modules

This guide is designed for LLMs and developers to systematically create robust, maintainable, and friendly tests for any sub-module in this codebase. üòä

---

## 1. Understand the Sub-Module
- Read the sub-module‚Äôs code to identify its classes, functions, and main responsibilities.
- Review its public API and any relevant docstrings or documentation.
- Identify dependencies and side effects (e.g., database, network, file system).

## 2. Review Existing Tests and Conventions
- Check existing test files for style, structure, and best practices.
- Note how fixtures, setup/teardown, and assertions are handled.
- Adopt conventions such as naming, logging, and error handling.

## 3. Identify Test Scenarios
- List all key functionalities and edge cases for the sub-module.
- For each function/class, consider:
  - Typical (happy path) usage
  - Boundary conditions and edge cases
  - Error handling and exceptions
  - Integration with other modules (if relevant)

## 4. Create a Step-by-Step Test Scenario
- For each identified scenario, write a clear, sequential outline of the test steps.
- Include setup, actions, assertions, and teardown for each scenario.
- Use descriptive language and, where possible, friendly or emoji-enhanced steps.
- Example:
  1. Prepare required environment/configuration.
  2. Initialize the object or system under test.
  3. Perform the main action (e.g., call a method, trigger an event).
  4. Assert the expected outcome or state.
  5. Clean up or reset any changed state.
- This step-by-step scenario will serve as your blueprint for implementing the actual test functions.

## 4. Prepare Test Data and Environment
- Define constants and test data at the top of the test file.
- Use environment variables or configuration files for sensitive data.
- Create pytest fixtures for setup and teardown of resources.

## 5. Write Test Functions
- Write one test function per scenario, using descriptive names.
- Use fixtures to manage setup/teardown.
- Add friendly, informative print/log statements (emojis encouraged!).
- Use clear, assertive checks for all expected outcomes.
- Handle and assert exceptions where appropriate.

## 6. Ensure Isolation and Repeatability
- Avoid shared state between tests.
- Clean up any resources (files, connections) after each test.
- Use mocking for external dependencies if needed.

## 7. Follow Documentation and Best Practices
- Refer to your project‚Äôs testing documentation for any specific requirements (e.g., `docs/development/testing.md`).
- Ensure your tests are readable, maintainable, and minimal.

## 8. Run and Validate Tests
- Run the test suite locally and ensure all tests pass.
- Fix any failures or flaky tests.
- Review test output for clarity and usefulness.

## 9. Document and Review
- Add docstrings or comments if a test‚Äôs purpose isn‚Äôt obvious.
- Ask for code review or run automated checks if your workflow supports it.

---

### Example Test Function Skeleton

```python
def test_some_functionality(submodule_fixture):
    print("üîç Testing some functionality...")
    result = submodule_fixture.some_method(args)
    assert result == expected
    print("‚úÖ Functionality works as expected!")
```

---

By following these steps, an LLM (or a human developer!) can systematically create high-quality, robust tests for any sub-module in your codebase. If you want a template or example for a specific sub-module, just ask! üöÄ
